import streamlit as st
import google.generativeai as genai

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Gemini 3 Master App", layout="centered")

# --- Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ù†Ù…Ø§Ø· ---
with st.sidebar:
    st.header("ğŸ® Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…")
    api_key = st.text_input("Gemini API Key", type="password")
    
    # Ù‡Ù†Ø§ Ù…Ø±Ø¨Ø· Ø§Ù„ÙØ±Ø³: Ø§Ø®ØªÙŠØ§Ø± "Ø§Ù„Ù†Ù…Ø·" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù…Ø¬Ø±Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    mode = st.radio(
        "Ø§Ø®ØªØ± Ù†Ù…Ø· Ø§Ù„Ø¹Ù…Ù„ (Mode):",
        ["ğŸ’¬ Chat (Ø§Ù„Ø¹Ø§Ø¯ÙŠ)", 
         "ğŸ§  Deep Thinking (ØªÙÙƒÙŠØ± Ø¹Ù…ÙŠÙ‚)", 
         "ğŸŒ Deep Research (Ø¨Ø­Ø« Ø¹Ù…ÙŠÙ‚)", 
         "ğŸ“ Canvas (Ø¨Ø±Ù…Ø¬Ø©/ÙƒØªØ§Ø¨Ø©)"]
    )

# --- ØªØ¹Ø±ÙŠÙ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… (System Instructions) ---
# Ù‡Ø°Ù‡ Ù‡ÙŠ "Ø§Ù„Ø£Ø¯Ù…ØºØ©" Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø§Ù„ØªÙŠ Ø³ØªØ¨Ø¯Ù„ Ø¨ÙŠÙ†Ù‡Ø§
system_prompts = {
    "ğŸ’¬ Chat (Ø§Ù„Ø¹Ø§Ø¯ÙŠ)": """
        Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯. Ø£Ø¬Ø¨ Ø¨Ø§Ø®ØªØµØ§Ø± ÙˆÙˆØ¶ÙˆØ­.
    """,
    
    "ğŸ§  Deep Thinking (ØªÙÙƒÙŠØ± Ø¹Ù…ÙŠÙ‚)": """
        ACT AS A REASONING ENGINE.
        Do not answer immediately. You must use "Chain of Thought" reasoning.
        1. Break the user's problem into small logical steps.
        2. Analyze each step critically.
        3. Verify your assumptions.
        4. Finally, provide the solution based on this deep analysis.
        Show your reasoning process clearly.
    """,
    
    "ğŸŒ Deep Research (Ø¨Ø­Ø« Ø¹Ù…ÙŠÙ‚)": """
        ACT AS A SENIOR ACADEMIC RESEARCHER.
        Your goal is to provide comprehensive, fact-based reports.
        - Prioritize accuracy over speed.
        - Cite sources/references for your claims.
        - If the topic is scientific (e.g., Petrophysics), use technical terminology correctly.
        - Compare different viewpoints.
    """,
    
    "ğŸ“ Canvas (Ø¨Ø±Ù…Ø¬Ø©/ÙƒØªØ§Ø¨Ø©)": """
        ACT AS A SENIOR PYTHON DEVELOPER & TECHNICAL WRITER.
        - Focus on generating production-ready code.
        - Do not use conversational fillers (like "Here is the code").
        - Output clean, commented code blocks.
        - If asked to write text, use structured Markdown with headers and bullet points.
    """
}

# --- ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ---
st.title(f"Gemini 3: {mode}")

if api_key:
    # 1. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ "Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ø®ØªØ§Ø±"
    genai.configure(api_key=api_key)
    
    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„Ù‚Ø§Ù…ÙˆØ³
    current_instruction = system_prompts[mode]
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª (System Instruction)
    model = genai.GenerativeModel(
        model_name="gemini-1.5-pro", # Ù†Ø³ØªØ®Ø¯Ù… Ø£Ù‚ÙˆÙ‰ Ù†Ù…ÙˆØ°Ø¬ Ø¯Ø§Ø¦Ù…Ø§Ù‹
        system_instruction=current_instruction 
    )

    # 2. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø´Ø§Øª
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Ø£Ø¯Ø®Ù„ Ø£Ù…Ø±Ùƒ Ù‡Ù†Ø§..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        try:
            with st.chat_message("assistant"):
                response_placeholder = st.empty()
                
                # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© (Ù„Ø§Ø­Ø¸ Ø£Ù†Ù†Ø§ Ù„Ø§ Ù†Ø­ØªØ§Ø¬ Ù„Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø®ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©ØŒ Ù„Ø£Ù†Ù‡ ØªÙ… ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù‡)
                # Ù†Ù‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ù„ØµÙŠØºØ© Ø§Ù„ØªÙŠ ÙŠÙÙ‡Ù…Ù‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                chat = model.start_chat(history=[]) 
                
                # (Ù…Ù„Ø§Ø­Ø¸Ø©: Ù„ØªØ¨Ø³ÙŠØ· Ø§Ù„ÙƒÙˆØ¯ Ù‡Ù†Ø§ Ù„Ù… Ù†Ù†Ù‚Ù„ ÙƒÙ„ Ø§Ù„Ù‡ÙŠØ³ØªÙˆØ±ÙŠØŒ Ù„ÙƒÙ† ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙƒØ§Ù…Ù„ ÙŠØ¬Ø¨ Ù†Ù‚Ù„Ù‡Ø§)
                response = chat.send_message(prompt, stream=True)
                
                full_text = ""
                for chunk in response:
                    if chunk.text:
                        full_text += chunk.text
                        response_placeholder.markdown(full_text + "â–Œ")
                
                response_placeholder.markdown(full_text)
                
            st.session_state.messages.append({"role": "assistant", "content": full_text})

        except Exception as e:
            st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")
